{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will use the following notebook to demonstrate different steps in preprocessing\n",
    "\n",
    "## These steps will include:\n",
    "\n",
    "### 1) Slice timing correction\n",
    "### 2) Motion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Import new things that we'll need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nipype.interfaces.afni as afni\n",
    "import nipype.interfaces.fsl as fsl\n",
    "import nipype.interfaces.freesurfer as fs\n",
    "from nipype.interfaces.utility import Function\n",
    "import seaborn as sns\n",
    "import nibabel as nb\n",
    "import json\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.pipeline.engine as pe \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I next want to get a list of all of my functional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = ['021']\n",
    "base_dir = '/home/dcarb040/Mattfeld_PSB6351'\n",
    "work_dir = '/scratch/classroom/psb6351/dcarb040/try2'\n",
    "func_dir = os.path.join(base_dir, f'dset/sub-{sid[0]}/ses-1/func')\n",
    "fmap_dir = os.path.join(base_dir, f'dset/sub-{sid[0]}/ses-1/fmap')\n",
    "fs_dir = os.path.join(base_dir, 'derivatives', 'freesurfer')\n",
    "\n",
    "# Get a list of my study task json and nifti converted files\n",
    "func_json = sorted(glob(func_dir + '/*.json'))\n",
    "func_files = sorted(glob(func_dir + '/*.nii.gz'))\n",
    "fmap_files = sorted(glob(fmap_dir + '/*func*.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next I want to build and run function to perform slice timing correction. I'm going to have to extract some important information from the .json files like the multiband slicetiming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221128-15:59:48,234 nipype.workflow INFO:\n",
      "\t Workflow psb6351_wf settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "221128-15:59:48,380 nipype.workflow INFO:\n",
      "\t Running in parallel.\n",
      "221128-15:59:48,400 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[2] jobs Slots[inf]\n",
      "221128-15:59:48,403 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.id_outliers ID: 0\n",
      "221128-15:59:49,79 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.id_outliers ID: 0\n",
      "221128-15:59:49,102 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.getsubs ID: 1\n",
      "221128-15:59:49,334 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.getsubs ID: 1\n",
      "221128-16:00:12,873 nipype.workflow INFO:\n",
      "\t [Job 1] Completed (psb6351_wf.getsubs).\n",
      "221128-16:00:26,681 nipype.workflow INFO:\n",
      "\t [Job 0] Completed (psb6351_wf.id_outliers).\n",
      "221128-16:00:26,721 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221128-16:00:26,734 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.getbestvol ID: 2\n",
      "221128-16:00:26,978 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.getbestvol ID: 2\n",
      "221128-16:00:30,689 nipype.workflow INFO:\n",
      "\t [Job 2] Completed (psb6351_wf.getbestvol).\n",
      "221128-16:00:30,715 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221128-16:00:30,734 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.extractref ID: 3\n",
      "221128-16:00:30,956 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.extractref ID: 3\n",
      "221128-16:00:50,714 nipype.workflow INFO:\n",
      "\t [Job 3] Completed (psb6351_wf.extractref).\n",
      "221128-16:00:50,753 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[2] jobs Slots[inf]\n",
      "221128-16:00:50,806 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.afni_register ID: 5\n",
      "221128-16:00:51,48 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.afni_register ID: 5\n",
      "221128-16:00:51,71 nipype.workflow INFO:\n",
      "\t Pending[1] Submitting[5] jobs Slots[inf]\n",
      "221128-16:00:51,93 nipype.workflow INFO:\n",
      "\t Submitting: _volreg0 ID: 9\n",
      "221128-16:00:51,344 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg0 ID: 9\n",
      "221128-16:00:51,367 nipype.workflow INFO:\n",
      "\t Submitting: _volreg1 ID: 10\n",
      "221128-16:00:51,598 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg1 ID: 10\n",
      "221128-16:00:51,620 nipype.workflow INFO:\n",
      "\t Submitting: _volreg2 ID: 11\n",
      "221128-16:00:51,863 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg2 ID: 11\n",
      "221128-16:00:51,885 nipype.workflow INFO:\n",
      "\t Submitting: _volreg3 ID: 12\n",
      "221128-16:00:52,133 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg3 ID: 12\n",
      "221128-16:00:52,156 nipype.workflow INFO:\n",
      "\t Submitting: _volreg4 ID: 13\n",
      "221128-16:00:52,413 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg4 ID: 13\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Mattfeld_PSB6351/.envs/psb6351_environment/lib/python3.9/site-packages/nipype/pipeline/plugins/tools.py:27\u001b[0m, in \u001b[0;36mreport_crash\u001b[0;34m(node, traceback, hostname)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Mattfeld_PSB6351/.envs/psb6351_environment/lib/python3.9/site-packages/nipype/pipeline/engine/nodes.py:223\u001b[0m, in \u001b[0;36mNode.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"Get result from result file (do not hold it in memory)\"\"\"\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_resultfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m.pklz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Mattfeld_PSB6351/.envs/psb6351_environment/lib/python3.9/site-packages/nipype/pipeline/engine/utils.py:291\u001b[0m, in \u001b[0;36mload_resultfile\u001b[0;34m(results_file, resolve)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results_file\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(results_file)\n\u001b[1;32m    293\u001b[0m result \u001b[38;5;241m=\u001b[39m loadpkl(results_file)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /scratch/classroom/psb6351/dcarb040/try2/psb6351workdir/sub-021/psb6351_wf/afni_register/result_afni_register.pklz",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 144\u001b[0m\n\u001b[1;32m    140\u001b[0m psb6351_wf\u001b[38;5;241m.\u001b[39mconnect(merge, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_file\u001b[39m\u001b[38;5;124m'\u001b[39m, datasink, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblur_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# The following two lines set a work directory outside of my \u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# local git repo and runs the workflow\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m psb6351_wf\u001b[38;5;241m.\u001b[39mrun(plugin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSLURM\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    145\u001b[0m plugin_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msbatch_args\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--partition classroom --qos pq_psb6351 --account acc_psb6351\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    146\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m#datasink \u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m#psb6351_wf.connect(afni_register, 'epi_al_mat', datasink, 'afniregister')\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#node setting up matrix\u001b[39;00m\n\u001b[1;32m    152\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/dcarb040/Mattfeld_PSB6351\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Mattfeld_PSB6351/.envs/psb6351_environment/lib/python3.9/site-packages/nipype/pipeline/engine/workflows.py:638\u001b[0m, in \u001b[0;36mWorkflow.run\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str2bool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_report\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_report_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, execgraph)\n\u001b[0;32m--> 638\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdatehash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdatehash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m datestr \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str2bool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite_provenance\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "File \u001b[0;32m~/Mattfeld_PSB6351/.envs/psb6351_environment/lib/python3.9/site-packages/nipype/pipeline/plugins/base.py:166\u001b[0m, in \u001b[0;36mDistributedPluginBase.run\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    165\u001b[0m         notrun\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 166\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m         )\n\u001b[1;32m    168\u001b[0m         errors\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Mattfeld_PSB6351/.envs/psb6351_environment/lib/python3.9/site-packages/nipype/pipeline/plugins/base.py:242\u001b[0m, in \u001b[0;36mDistributedPluginBase._clean_queue\u001b[0;34m(self, jobid, graph, result)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(format_exception(\u001b[38;5;241m*\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info())),\n\u001b[1;32m    240\u001b[0m     }\n\u001b[0;32m--> 242\u001b[0m crashfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_report_crash\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mjobid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str2bool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_on_first_crash\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/Mattfeld_PSB6351/.envs/psb6351_environment/lib/python3.9/site-packages/nipype/pipeline/plugins/base.py:226\u001b[0m, in \u001b[0;36mDistributedPluginBase._report_crash\u001b[0;34m(self, node, result)\u001b[0m\n\u001b[1;32m    224\u001b[0m     tb \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    225\u001b[0m     node\u001b[38;5;241m.\u001b[39m_traceback \u001b[38;5;241m=\u001b[39m tb\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreport_crash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Mattfeld_PSB6351/.envs/psb6351_environment/lib/python3.9/site-packages/nipype/pipeline/plugins/tools.py:29\u001b[0m, in \u001b[0;36mreport_crash\u001b[0;34m(node, traceback, hostname)\u001b[0m\n\u001b[1;32m     27\u001b[0m         result \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mresult\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m         traceback \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124mWhen creating this crashfile, the results file corresponding\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124mto the node could not be found.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39msplitlines(\n\u001b[1;32m     33\u001b[0m             keepends\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         )\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     36\u001b[0m         traceback \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124mDuring the creation of this crashfile triggered by the above exception,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m             keepends\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "# Here I am building a function that eliminates the\n",
    "# mapnode directory structure and assists in saving\n",
    "# all of the outputs into a single directory\n",
    "def get_subs(func_files):\n",
    "    '''Produces Name Substitutions for Each Contrast'''\n",
    "    subs = []\n",
    "    for curr_run in range(len(func_files)):\n",
    "        subs.append(('_tshifter%d' %curr_run, ''))\n",
    "        subs.append(('_volreg%d' %curr_run, ''))\n",
    "    return subs\n",
    "\n",
    "# Here I am building a function that takes in a\n",
    "# text file that includes the number of outliers\n",
    "# at each volume and then finds which volume (e.g., index)\n",
    "# has the minimum number of outliers (e.g., min) \n",
    "# searching over the first 201 volumes\n",
    "# If the index function returns a list because there were\n",
    "# multiple volumes with the same outlier count, pick the first one\n",
    "def best_vol(outlier_count):\n",
    "    best_vol_num = outlier_count.index(min(outlier_count[:200]))\n",
    "    if isinstance(best_vol_num, list):\n",
    "        best_vol_num = best_vol_num[0]\n",
    "    return best_vol_num\n",
    "\n",
    "# Here I am creating a list of lists containing the slice timing for each study run\n",
    "slice_timing_list = []\n",
    "for curr_json in func_json:\n",
    "    curr_json_data = open(curr_json)\n",
    "    curr_func_metadata = json.load(curr_json_data)\n",
    "    slice_timing_list.append(curr_func_metadata['SliceTiming'])\n",
    "\n",
    "# Here I am establishing a nipype work flow that I will eventually execute\n",
    "psb6351_wf = pe.Workflow(name='psb6351_wf')\n",
    "psb6351_wf.base_dir = work_dir + f'/psb6351workdir/sub-{sid[0]}'\n",
    "psb6351_wf.config['execution']['use_relative_paths'] = True\n",
    "\n",
    "# Create a Function node to substitute names of files created during pipeline\n",
    "getsubs = pe.Node(Function(input_names=['func_files'],\n",
    "                           output_names=['subs'],\n",
    "                           function=get_subs),\n",
    "                  name='getsubs')\n",
    "getsubs.inputs.func_files = func_files\n",
    "\n",
    "# Here I am inputing just the first run functional data\n",
    "# I want to use afni's 3dToutcount to find the number of \n",
    "# outliers at each volume.  I will use this information to\n",
    "# later select the earliest volume with the least number of outliers\n",
    "# to serve as the base for the motion correction\n",
    "id_outliers = pe.Node(afni.OutlierCount(),\n",
    "                      name = 'id_outliers')\n",
    "id_outliers.inputs.in_file = func_files[0]\n",
    "id_outliers.inputs.automask = True\n",
    "id_outliers.inputs.out_file = 'outlier_file'\n",
    "\n",
    "# Create a Function node to identify the best volume based\n",
    "# on the number of outliers at each volume. I'm searching\n",
    "# for the index in the first 201 volumes that has the \n",
    "# minimum number of outliers and will use the min() function\n",
    "# I will use the index function to get the best vol. \n",
    "getbestvol = pe.Node(Function(input_names=['outlier_count'],\n",
    "                              output_names=['best_vol_num'],\n",
    "                              function=best_vol),\n",
    "                     name='getbestvol')\n",
    "psb6351_wf.connect(id_outliers, 'out_file', getbestvol, 'outlier_count')\n",
    "\n",
    "# Extract the earliest volume with the\n",
    "# the fewest outliers of the first run as the reference \n",
    "extractref = pe.Node(fsl.ExtractROI(t_size=1),\n",
    "                     name = \"extractref\")\n",
    "extractref.inputs.in_file = func_files[0]\n",
    "#extractref.inputs.t_min = int(np.ceil(nb.load(study_func_files[0]).shape[3]/2)) #PICKING MIDDLE\n",
    "psb6351_wf.connect(getbestvol, 'best_vol_num', extractref, 't_min')\n",
    "\n",
    "\n",
    "# Below is the command that runs AFNI's 3dvolreg command.\n",
    "# this is the node that performs the motion correction\n",
    "# I'm iterating over the functional files which I am passing\n",
    "# functional data from the slice timing correction node before\n",
    "# I'm using the earliest volume with the least number of outliers\n",
    "# during the first run as the base file to register to.\n",
    "\n",
    "volreg = pe.MapNode(afni.Volreg(),\n",
    "                    iterfield=['in_file'],\n",
    "                    name = 'volreg')\n",
    "volreg.inputs.outputtype = 'NIFTI_GZ'\n",
    "volreg.inputs.in_file = func_files \n",
    "psb6351_wf.connect(extractref, 'roi_file', volreg, 'basefile')\n",
    "\n",
    "# Below is the command that runs AFNI's 3dTshift command\n",
    "# this is the node that performs the slice timing correction\n",
    "# I input the study func files as a list and the slice timing \n",
    "# as a list of lists. I'm using a MapNode to iterate over the two.\n",
    "# this should allow me to parallelize this on the HPC\n",
    "\n",
    "tshifter = pe.MapNode(afni.TShift(),\n",
    "                      iterfield=['in_file','slice_timing'],\n",
    "                      name = 'tshifter')\n",
    "tshifter.inputs.tr = '1.76'\n",
    "tshifter.inputs.slice_timing = slice_timing_list\n",
    "tshifter.inputs.outputtype = 'NIFTI_GZ'\n",
    "tshifter.inputs.in_file = func_files\n",
    "psb6351_wf.connect(volreg, 'out_file', tshifter, 'in_file')\n",
    "\n",
    "\n",
    "#Below is the coregistration step from homework 3. With this, we can overlay the functional and structural scans \n",
    "\n",
    "afni_register = pe.Node(afni.AlignEpiAnatPy(), name = 'afni_register')\n",
    "afni_register.inputs.anat = f'{base_dir}/dset/sub-{sid[0]}/ses-1/anat/sub-021_run-2_T1w.nii.gz'\n",
    "afni_register.inputs.epi2anat = True\n",
    "afni_register.epi_base = 0\n",
    "afni_register.outputtype = \"NIFTI_GZ\"\n",
    "psb6351_wf.connect(extractref, 'roi_file', afni_register, 'in_file')\n",
    "\n",
    "\n",
    "#below is the blurring step from homework 4\n",
    "\n",
    "#spatial smoothing\n",
    "\n",
    "merge = pe.MapNode(afni.Merge(), iterfield = [\"in_files\", \"blurfwhm\"], name = 'merge')\n",
    "merge.inputs.blurfwhm = [3,6,9,12,14,16]\n",
    "merge.inputs.doall=True\n",
    "psb6351_wf.connect(tshifter, 'out_file', merge, 'in_files')\n",
    "\n",
    "\n",
    "# Below is the node that collects all the data and saves\n",
    "# the outputs that I am interested in. Here in this node\n",
    "# I use the substitutions input combined with the earlier\n",
    "# function to get rid of nesting\n",
    "datasink = pe.Node(nio.DataSink(), name=\"datasink\")\n",
    "datasink.inputs.base_directory = os.path.join(base_dir, 'derivatives/preproc')\n",
    "datasink.inputs.container = f'sub-{sid[0]}'\n",
    "psb6351_wf.connect(tshifter, 'out_file', datasink, 'sltime_corr')\n",
    "psb6351_wf.connect(extractref, 'roi_file', datasink, 'study_ref')\n",
    "psb6351_wf.connect(volreg, 'out_file', datasink, 'motion.@corrfile')\n",
    "psb6351_wf.connect(volreg, 'oned_matrix_save', datasink, 'motion.@matrix')\n",
    "psb6351_wf.connect(volreg, 'oned_file', datasink, 'motion.@par')\n",
    "psb6351_wf.connect(getsubs, 'subs', datasink, 'substitutions')\n",
    "psb6351_wf.connect(afni_register,'epi_al_mat', datasink, 'afniregister')\n",
    "psb6351_wf.connect(merge, 'out_file', datasink, 'merge_file')\n",
    "psb6351_wf.connect(merge, 'out_file', datasink, 'blur_step')\n",
    "\n",
    "# The following two lines set a work directory outside of my \n",
    "# local git repo and runs the workflow\n",
    "psb6351_wf.run(plugin='SLURM',\n",
    "plugin_args={'sbatch_args': ('--partition classroom --qos pq_psb6351 --account acc_psb6351'),\n",
    "                            'overwrite':True})\n",
    "\n",
    "#datasink \n",
    "#psb6351_wf.connect(afni_register, 'epi_al_mat', datasink, 'afniregister')\n",
    "\n",
    "#node setting up matrix\n",
    "base_dir = '/home/dcarb040/Mattfeld_PSB6351'\n",
    "reg_file = glob(base_dir + 'derivatives/preproc/sub-021/afniregister/*')\n",
    "                \n",
    "#hw 3 matrix node \n",
    "reg_mtrx = pd.read_csv(reg_file[0], header = None)\n",
    "print(reg_mtrx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I will load and plot the motion files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_dir = os.path.join(base_dir, f'derivatives/preproc/sub-{sid[0]}/motion')\n",
    "study_motion_files = sorted(glob(motion_dir + '/*study*_bold_tshift.1D'))\n",
    "\n",
    "for curr_mot_file in study_motion_files:\n",
    "    motion_df = pd.read_csv(curr_mot_file, sep=\"  \", header=None)\n",
    "    motion_df.columns = ['roll', 'pitch', 'yaw', 'dS', 'dL', 'dP']\n",
    "\n",
    "    num_vols = range(1, len(motion_df)+1)\n",
    "    fig, axs = plt.subplots(motion_df.shape[1], 1, figsize = (15, 10))\n",
    "    # make a little extra space between the subplots\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    for idx, curr_col in enumerate(motion_df.keys()):\n",
    "        axs[idx].plot(num_vols, motion_df[f'{curr_col}'])\n",
    "        axs[idx].set_xlabel('TRs')\n",
    "        axs[idx].set_ylabel(f'{curr_col}')\n",
    "        axs[idx].grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_motcorr_files = sorted(glob(motion_dir + '/*.nii.gz'))\n",
    "study_motcorr_img_data = nb.load(study_motcorr_files[0]).get_fdata()\n",
    "study_orig_img_data = nb.load(func_files[0]).get_fdata()\n",
    "\n",
    "#test_motcorr_img_data.shape\n",
    "\n",
    "print(study_motcorr_img_data[50,50,32,50])\n",
    "print(study_orig_img_data[50,50,32,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_file = glob(base_dir + '/derivatives/preproc/blur/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_matrix = pd.read_table(reg_file[0], header=None)\n",
    "print(reg_matrix)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
